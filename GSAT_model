import os, random
import numpy as np
import pandas as pd
from tqdm.auto import tqdm
import warnings

warnings.filterwarnings('ignore')

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from torch_geometric.data import Data, Batch
from torch_geometric.nn.pool import global_mean_pool
from torch.optim.swa_utils import AveragedModel, update_bn

from rdkit import Chem, RDLogger
from rdkit.Chem import AllChem, Crippen, Descriptors, rdPartialCharges
from rdkit.Chem.Scaffolds import MurckoScaffold

import periodictable

# suppress RDKit logs
RDLogger.DisableLog('rdApp.*')
os.environ["PYTHONWARNINGS"] = "ignore"

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

# 1) Load SMILES + LC50 only
df = pd.read_csv(
    '/Users/suppboat/PycharmProjects/Toxicity/Copy of Merged_Norman-M_DS.csv',
    usecols=['SMILES', 'LC50[-LOG(mol/L)]'],
    dtype={'SMILES': str, 'LC50[-LOG(mol/L)]': float},
    low_memory=False
)
df.rename(columns={'LC50[-LOG(mol/L)]': 'LC50'}, inplace=True)

# NORMALIZE TARGETS to prevent NaN loss
y_mean, y_std = df.LC50.mean(), df.LC50.std()
df['LC50_norm'] = (df.LC50 - y_mean) / y_std
print(f"Target stats: mean={y_mean:.3f}, std={y_std:.3f}")
print(f"Normalized target range: [{df.LC50_norm.min():.3f}, {df.LC50_norm.max():.3f}]")


# 2) Precompute & cache conformers
def compute_and_cache_conformers(smiles, cache='confs.npz',
                                 max_conf=3, max_attempts=10):
    if os.path.exists(cache):
        npz = np.load(cache, allow_pickle=True)
        return [npz[f'arr_{i}'] for i in range(len(smiles))]

    all_confs = []
    for smi in tqdm(smiles, desc='Embedding conformers'):
        m0 = Chem.MolFromSmiles(smi)
        confs = []
        if m0:
            try:
                Chem.SanitizeMol(m0)
                for _ in range(max_conf):
                    m = Chem.AddHs(m0)
                    for k in range(max_attempts):
                        if AllChem.EmbedMolecule(m, randomSeed=SEED + k) == 0:
                            break
                    props = AllChem.MMFFGetMoleculeProperties(m)
                    AllChem.MMFFOptimizeMolecule(m, mmffProps=props)
                    m3 = Chem.RemoveHs(m)
                    conf = m3.GetConformer()
                    N = m3.GetNumAtoms()
                    pts = np.zeros((N, 3), dtype=np.float32)
                    for i in range(N):
                        p = conf.GetAtomPosition(i)
                        pts[i] = (p.x, p.y, p.z)
                    confs.append(pts)
            except:
                pass
        all_confs.append(confs or [None])

    np.savez_compressed(cache, *all_confs)
    return all_confs


conformers_list = compute_and_cache_conformers(df.SMILES.values)


# 3) Featurizers & Tokenizer
class Featurizer:
    def __init__(self, sets):
        self.dim = 0
        self.map = {}
        for k, v in sets.items():
            vs = sorted(v)
            self.map[k] = {val: i + self.dim for i, val in enumerate(vs)}
            self.dim += len(vs)

    def encode(self, obj):
        vec = np.zeros(self.dim, dtype=np.float32)
        for k, m in self.map.items():
            v = getattr(self, k)(obj)
            idx = m.get(v)
            if idx is not None: vec[idx] = 1.0
        return vec


class AtomFeaturizer(Featurizer):
    def symbol(self, a): return a.GetSymbol()

    def n_valence(self, a): return a.GetTotalValence()

    def n_hydrogens(self, a): return a.GetTotalNumHs()

    def hybridization(self, a): return a.GetHybridization().name.lower()

    def formal_charge(self, a): return a.GetFormalCharge()

    def degree(self, a): return a.GetDegree()

    def explicit_valence(self, a): return a.GetExplicitValence()

    def implicit_valence(self, a): return a.GetImplicitValence()

    def num_radical_electrons(self, a): return a.GetNumRadicalElectrons()

    def total_degree(self, a): return a.GetTotalDegree()

    def atom_mass(self, a): return round(a.GetMass())


class BondFeaturizer(Featurizer):
    def __init__(self, sets):
        super().__init__(sets)
        self.dim += 1

    def encode(self, bond):
        if bond is None:
            z = np.zeros(self.dim, dtype=np.float32)
            z[-1] = 1.0
            return z
        return super().encode(bond)

    def bond_type(self, b): return b.GetBondType().name.lower()

    def conjugated(self, b): return b.GetIsConjugated()

    def stereo(self, b): return b.GetStereo().name.lower()

    def same_ring(self, b): return b.IsInRing()

    def ring_membership(self, b):
        ri = b.GetOwningMol().GetRingInfo()
        return ri.NumBondRings(b.GetIdx())


class SMILESTokenizer:
    def __init__(self, smiles):
        chars = sorted({ch for s in smiles for ch in s})
        self.t2i = {c: i + 1 for i, c in enumerate(chars)}
        self.t2i['<pad>'] = 0

    def encode(self, s): return [self.t2i[ch] for ch in s]

    @property
    def vocab_size(self): return len(self.t2i)


atom_fs = AtomFeaturizer({
    'symbol': {e.symbol for e in periodictable.elements},
    'n_valence': set(range(7)),
    'n_hydrogens': set(range(5)),
    'hybridization': {'s', 'sp', 'sp2', 'sp3'},
    'formal_charge': set(range(-3, 4)),
    'degree': set(range(7)),
    'explicit_valence': set(range(7)),
    'implicit_valence': set(range(7)),
    'num_radical_electrons': set(range(7)),
    'total_degree': set(range(7)),
    'atom_mass': set(range(1, 251))
})
bond_fs = BondFeaturizer({
    'bond_type': {'single', 'double', 'triple', 'aromatic'},
    'conjugated': {True, False},
    'stereo': {'none', 'cis', 'trans', 'any'},
    'same_ring': {True, False},
    'ring_membership': set(range(7))
})
tokenizer = SMILESTokenizer(df.SMILES.values)


# 4) Dataset & collate
class MoleculeDataset(Dataset):
    def __init__(self, df, confs, atom_fs, bond_fs, tok, y_mean, y_std, cutoff=5.0):
        self.smiles = df.SMILES.values
        self.y = df.LC50.values.astype(np.float32)
        self.y_mean, self.y_std = y_mean, y_std
        self.confs = confs
        self.atom_fs, self.bond_fs, self.tok = atom_fs, bond_fs, tok
        self.cutoff = cutoff

    def __len__(self):
        return len(self.smiles)

    def __getitem__(self, idx):
        smi = self.smiles[idx]
        y = (self.y[idx] - self.y_mean) / self.y_std  # Normalize target here

        mol = Chem.MolFromSmiles(smi)
        Chem.SanitizeMol(mol)
        rdPartialCharges.ComputeGasteigerCharges(mol)

        # Handle NaN partial charges
        pc = []
        for a in mol.GetAtoms():
            if a.HasProp('_GasteigerCharge'):
                charge = float(a.GetProp('_GasteigerCharge'))
                pc.append(charge if not np.isnan(charge) else 0.0)
            else:
                pc.append(0.0)
        pc = np.array(pc, dtype=np.float32)

        # Handle NaN in descriptors and normalize
        tpsa = Descriptors.TPSA(mol)
        molwt = Descriptors.MolWt(mol)
        logp = Descriptors.MolLogP(mol)

        if np.isnan(tpsa): tpsa = 0.0
        if np.isnan(molwt): molwt = 200.0
        if np.isnan(logp): logp = 0.0

        g_feats = np.array([tpsa / 100.0, molwt / 500.0, logp / 5.0], dtype=np.float32)

        atom_feats = np.vstack([self.atom_fs.encode(a) for a in mol.GetAtoms()])
        atom_feats = np.concatenate([atom_feats, pc[:, None]], axis=1)
        feat_dim = atom_feats.shape[1]

        # Murcko scaffold
        sc = MurckoScaffold.GetScaffoldForMol(mol)
        sc_atoms = [np.concatenate([self.atom_fs.encode(a), [0.0]]) for a in sc.GetAtoms()]
        if not sc_atoms:
            sc_atoms = [np.zeros(feat_dim, dtype=np.float32)]
            sei, sea = [(0, 0)], [np.concatenate([self.bond_fs.encode(None), [0.0]])]
        else:
            sei, sea = [], []
            for b in sc.GetBonds():
                u, v = b.GetBeginAtomIdx(), b.GetEndAtomIdx()
                fe = np.concatenate([self.bond_fs.encode(b), [0.0]])
                sei += [(u, v), (v, u)]
                sea += [fe, fe]
        scaffold = Data(
            x=torch.tensor(np.vstack(sc_atoms), dtype=torch.float32),
            edge_index=torch.tensor(sei, dtype=torch.long).t().contiguous(),
            edge_attr=torch.tensor(sea, dtype=torch.float32)
        )
        scaffold.pos = torch.zeros((scaffold.x.size(0), 3), dtype=torch.float32)

        # conformer graphs
        confs, N = self.confs[idx], mol.GetNumAtoms()
        graphs = []
        for coords in confs:
            ei, ea = [], []
            for u in range(N):
                ei.append((u, u))
                ea.append(np.concatenate([self.bond_fs.encode(None), [0.0]]))
            for u in range(N):
                for v in range(N):
                    if u == v: continue
                    d = 0.0
                    if coords is not None:
                        d = float(np.linalg.norm(coords[u] - coords[v]))
                        d = min(d, self.cutoff)
                        if d > self.cutoff: continue
                    bnd = mol.GetBondBetweenAtoms(u, v)
                    fe = np.concatenate([self.bond_fs.encode(bnd), [d]])
                    ei.append((u, v))
                    ea.append(fe)
            g = Data(
                x=torch.tensor(atom_feats, dtype=torch.float32),
                edge_index=torch.tensor(ei, dtype=torch.long).t().contiguous(),
                edge_attr=torch.tensor(ea, dtype=torch.float32)
            )
            if coords is not None:
                g.pos = torch.tensor(coords, dtype=torch.float32)
            else:
                g.pos = torch.zeros((N, 3), dtype=torch.float32)
            graphs.append(g)

        toks = torch.tensor(self.tok.encode(smi), dtype=torch.long)
        return graphs, scaffold, toks, torch.tensor(y, dtype=torch.float32), torch.tensor(g_feats, dtype=torch.float32)


def collate_fn(batch):
    graphs_list, scaffs, toks, ys, gf = zip(*batch)
    scaffold_batch = Batch.from_data_list(scaffs)
    flat, counts = [], []
    for gl in graphs_list:
        flat += gl
        counts.append(len(gl))
    conf_batch = Batch.from_data_list(flat)
    toks_pad = nn.utils.rnn.pad_sequence(toks, batch_first=True, padding_value=0)
    return conf_batch, torch.tensor(counts), scaffold_batch, toks_pad, torch.stack(ys), torch.stack(gf)


# 5) Model components (unchanged from previous version)
class GaussianRBF(nn.Module):
    def __init__(self, K=32, cutoff=5.0):
        super().__init__()
        centers = torch.linspace(0, cutoff, K)
        self.gamma = (centers[1] - centers[0]).item() ** -2
        self.register_buffer('centers', centers)

    def forward(self, d):
        d = torch.clamp(d, min=0.0, max=10.0)
        return torch.exp(-self.gamma * (d.unsqueeze(-1) - self.centers) ** 2)


class EdgeNetwork(nn.Module):
    def __init__(self, in_dim, emb):
        super().__init__()
        self.lin = nn.Linear(in_dim, emb * emb)
        self.norm = nn.LayerNorm(emb)
        nn.init.xavier_uniform_(self.lin.weight)
        nn.init.zeros_(self.lin.bias)

    def forward(self, h, ei, ea):
        M, E = ei.size(1), h.size(1)
        m = self.lin(ea).view(M, E, E)
        hj = h[ei[1]].unsqueeze(-1)
        m = (m @ hj).squeeze(-1)
        agg = torch.zeros_like(h).index_add(0, ei[0], m)
        return self.norm(h + agg)


class DistanceSelfAttention(nn.Module):
    def __init__(self, emb, heads, drop):
        super().__init__()
        self.h, self.d = heads, emb // heads
        self.q = nn.Linear(emb, emb)
        self.k = nn.Linear(emb, emb)
        self.v = nn.Linear(emb, emb)
        self.out = nn.Linear(emb, emb)
        self.drop = nn.Dropout(drop)
        for module in [self.q, self.k, self.v, self.out]:
            nn.init.xavier_uniform_(module.weight)
            nn.init.zeros_(module.bias)

    def forward(self, x, db, mask):
        B, N, E = x.size()
        q = self.q(x).view(B, N, self.h, self.d).transpose(1, 2)
        k = self.k(x).view(B, N, self.h, self.d).transpose(1, 2)
        v = self.v(x).view(B, N, self.h, self.d).transpose(1, 2)
        sc = (q @ k.transpose(-2, -1)) / np.sqrt(self.d) + db.unsqueeze(1)
        sc = torch.clamp(sc, min=-10.0, max=10.0)
        if mask is not None:
            m = mask[:, None, None, :].bool()
            sc = sc.masked_fill(~m, float('-inf'))
        a = F.softmax(sc, dim=-1)
        a = self.drop(a)
        o = (a @ v).transpose(1, 2).contiguous().view(B, N, E)
        return self.out(o)


class GraphTransformerLayer(nn.Module):
    def __init__(self, emb, heads, drop):
        super().__init__()
        self.att = DistanceSelfAttention(emb, heads, drop)
        self.n1 = nn.LayerNorm(emb)
        self.ff = nn.Sequential(nn.Linear(emb, emb * 2), nn.ReLU(),
                                nn.Linear(emb * 2, emb), nn.Dropout(drop))
        self.n2 = nn.LayerNorm(emb)

    def forward(self, x, db, mask):
        h = self.att(x, db, mask)
        x1 = self.n1(x + h)
        h2 = self.ff(x1)
        return self.n2(x1 + h2)


class GraphEncoder(nn.Module):
    def __init__(self, atom_dim, bond_dim, emb, heads, layers, drop, rbf_K=32, cutoff=5.0):
        super().__init__()
        self.proj = nn.Linear(atom_dim, emb)
        self.rbf = GaussianRBF(rbf_K, cutoff)
        edge_in = bond_dim + rbf_K
        self.e_net = EdgeNetwork(edge_in, emb)
        self.layers = nn.ModuleList([GraphTransformerLayer(emb, heads, drop)
                                     for _ in range(layers)])
        nn.init.xavier_uniform_(self.proj.weight)
        nn.init.zeros_(self.proj.bias)

    def forward(self, x, ei, ea, pos, batch):
        h = self.proj(x)
        d = ea[:, -1]
        bf = ea[:, :-1]
        ea2 = torch.cat([bf, self.rbf(d)], dim=1)
        h = self.e_net(h, ei, ea2)
        B = batch.max().item() + 1
        xs, ds, ms = [], [], []

        for i in range(B):
            idx = (batch == i).nonzero(as_tuple=False).squeeze()
            if idx.dim() == 0: idx = idx.unsqueeze(0)
            hi, pi = h[idx], pos[idx]
            if pi.dim() == 1: pi = pi.unsqueeze(0)
            xs.append(hi)
            ms.append(torch.ones(hi.size(0), device=h.device, dtype=torch.bool))
            ds.append(torch.cdist(pi, pi))

        Nmax = max(xi.size(0) for xi in xs)
        x_pad = torch.stack([F.pad(xi, (0, 0, 0, Nmax - xi.size(0))) for xi in xs])
        d_pad = torch.stack([F.pad(di, (0, Nmax - di.size(1), 0, Nmax - di.size(0))) for di in ds])
        m_pad = torch.stack([F.pad(mi, (0, Nmax - mi.size(0))) for mi in ms])

        for lyr in self.layers:
            x_pad = lyr(x_pad, d_pad, m_pad)

        eps = 1e-8
        mf = m_pad.float().unsqueeze(-1)
        return (x_pad * mf).sum(1) / (mf.sum(1) + eps)


class SequenceEncoder(nn.Module):
    def __init__(self, vocab, emb, heads, hid, layers, drop):
        super().__init__()
        self.tok = nn.Embedding(vocab, emb, padding_idx=0)
        self.pos = nn.Embedding(256, emb)
        enc = nn.TransformerEncoderLayer(emb, heads, hid, drop)
        self.tr = nn.TransformerEncoder(enc, layers)
        nn.init.normal_(self.tok.weight, std=0.02)
        nn.init.normal_(self.pos.weight, std=0.02)

    def forward(self, toks):
        B, L = toks.size()
        p = torch.arange(L, device=toks.device).unsqueeze(0).expand(B, L)
        x = self.tok(toks) + self.pos(p)
        x = x.transpose(0, 1)
        return self.tr(x).mean(0)


class FusionGating(nn.Module):
    def __init__(self, emb, hid=64):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(2 * emb, hid), nn.ReLU(),
            nn.Linear(hid, emb), nn.Sigmoid()
        )
        for module in self.net:
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.zeros_(module.bias)

    def forward(self, g, s):
        gate = self.net(torch.cat([g, s], dim=-1))
        return gate * g + (1 - gate) * s


class CrossModal(nn.Module):
    def __init__(self, emb, heads, drop, layers=2):
        super().__init__()
        self.layers = nn.ModuleList([
            nn.ModuleList([
                nn.MultiheadAttention(emb, heads, dropout=drop),
                nn.LayerNorm(emb),
                nn.MultiheadAttention(emb, heads, dropout=drop),
                nn.LayerNorm(emb)
            ]) for _ in range(layers)
        ])

    def forward(self, g, s):
        for a1, n1, a2, n2 in self.layers:
            g2s, _ = a1(g.unsqueeze(0), s.unsqueeze(0), s.unsqueeze(0))
            g = n1(g + g2s.squeeze(0))
            s2g, _ = a2(s.unsqueeze(0), g.unsqueeze(0), g.unsqueeze(0))
            s = n2(s + s2g.squeeze(0))
        return (g + s) / 2


class MultiModalRegressor(nn.Module):
    def __init__(self, atom_dim, bond_dim, vocab_size,
                 emb=128, gh=8, gl=3, sh=4, sl=4, drop=0.1, gf_dim=3):
        super().__init__()
        self.ge = GraphEncoder(atom_dim, bond_dim, emb, gh, gl, drop)
        self.se = SequenceEncoder(vocab_size, emb, sh, emb * 2, sl, drop)
        self.fg = FusionGating(emb)
        self.cm = CrossModal(emb, sh, drop, layers=2)
        self.read = nn.Sequential(
            nn.Linear(emb + gf_dim, emb), nn.ReLU(), nn.Dropout(drop),
            nn.Linear(emb, 1)
        )
        for module in self.read:
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.zeros_(module.bias)

    def forward(self, conf_batch, counts, scaffold_batch, toks, g_feats):
        gb = self.ge(conf_batch.x, conf_batch.edge_index,
                     conf_batch.edge_attr, conf_batch.pos, conf_batch.batch)
        outs = []
        start = 0
        for c in counts:
            outs.append(gb[start:start + c].mean(0))
            start += c
        g_conf = torch.stack(outs)
        g_sc = self.ge(scaffold_batch.x,
                       scaffold_batch.edge_index,
                       scaffold_batch.edge_attr,
                       scaffold_batch.pos,
                       scaffold_batch.batch)
        g_tot = g_conf + g_sc
        s_emb = self.se(toks)
        f0 = self.fg(g_tot, s_emb)
        f = self.cm(f0, f0)
        cat = torch.cat([f, g_feats], dim=-1)
        return self.read(cat).squeeze(-1)

    def save(self, path, opt=None, sched=None):
        ck = {'model': self.state_dict()}
        if opt: ck['opt'] = opt.state_dict()
        if sched: ck['sched'] = sched.state_dict()
        torch.save(ck, path)

    @classmethod
    def load(cls, path, device='cpu', **kw):
        ck = torch.load(path, map_location=device)
        m = cls(**kw).to(device)
        m.load_state_dict(ck['model'])
        return m, ck.get('opt'), ck.get('sched')


# 6) Enhanced Training & evaluation with multiple metrics
def compute_metrics(predictions, targets):
    """Compute MSE, MAE, and R² metrics"""
    mse = F.mse_loss(predictions, targets, reduction='mean').item()
    mae = F.l1_loss(predictions, targets, reduction='mean').item()

    # R² = 1 - (SS_res / SS_tot)
    ss_res = torch.sum((targets - predictions) ** 2).item()
    ss_tot = torch.sum((targets - targets.mean()) ** 2).item()
    r2 = 1 - (ss_res / (ss_tot + 1e-8))  # Add epsilon to prevent division by zero

    return mse, mae, r2


def train_epoch(model, loader, opt, scaler, accum, dev, debug=False):
    model.train()
    total_loss = 0
    all_preds, all_targets = [], []

    pbar = tqdm(loader, desc='Train', leave=False)
    opt.zero_grad()

    for i, batch in enumerate(pbar):
        cb, counts, sb, toks, ys, gf = batch
        cb, counts, sb, toks, ys, gf = [x.to(dev) for x in (cb, counts, sb, toks, ys, gf)]

        # Debug first batch
        if debug and i == 0:
            print(f"Input checks:")
            print(f"  cb.x: {cb.x.shape}, range [{cb.x.min():.3f}, {cb.x.max():.3f}], nan: {torch.isnan(cb.x).any()}")
            print(f"  targets: {ys.shape}, range [{ys.min():.3f}, {ys.max():.3f}], nan: {torch.isnan(ys).any()}")

        with torch.cuda.amp.autocast():
            out = model(cb, counts, sb, toks, gf)

            if debug and i == 0:
                print(f"  output: {out.shape}, range [{out.min():.3f}, {out.max():.3f}], nan: {torch.isnan(out).any()}")

            loss = F.mse_loss(out, ys) / accum

            if torch.isnan(loss):
                print(f"NaN loss at batch {i}!")
                print(f"Outputs: {out}")
                print(f"Targets: {ys}")
                break

        scaler.scale(loss).backward()
        if (i + 1) % accum == 0:
            scaler.unscale_(opt)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
            scaler.step(opt)
            scaler.update()
            opt.zero_grad()

        total_loss += loss.item() * accum * ys.size(0)
        all_preds.append(out.detach())
        all_targets.append(ys.detach())

        # Update progress bar with running metrics
        if i > 0 and i % 50 == 0:  # Update every 50 batches
            temp_preds = torch.cat(all_preds[-50:])
            temp_targets = torch.cat(all_targets[-50:])
            temp_mse, temp_mae, temp_r2 = compute_metrics(temp_preds, temp_targets)
            pbar.set_postfix({
                'loss': total_loss / ((i + 1) * ys.size(0)),
                'mse': temp_mse,
                'mae': temp_mae,
                'r2': temp_r2
            })

    pbar.close()

    # Final metrics for entire epoch
    all_preds = torch.cat(all_preds)
    all_targets = torch.cat(all_targets)
    epoch_mse, epoch_mae, epoch_r2 = compute_metrics(all_preds, all_targets)

    return {
        'loss': total_loss / len(loader.dataset),
        'mse': epoch_mse,
        'mae': epoch_mae,
        'r2': epoch_r2
    }


def eval_epoch(model, loader, dev):
    model.eval()
    total_loss = 0
    all_preds, all_targets = [], []

    pbar = tqdm(loader, desc='Eval', leave=False)
    with torch.no_grad():
        for i, batch in enumerate(pbar):
            cb, counts, sb, toks, ys, gf = batch
            cb, counts, sb, toks, ys, gf = [x.to(dev) for x in (cb, counts, sb, toks, ys, gf)]
            out = model(cb, counts, sb, toks, gf)

            loss = F.mse_loss(out, ys, reduction='sum').item()
            total_loss += loss

            all_preds.append(out)
            all_targets.append(ys)

            # Update progress bar periodically
            if i > 0 and i % 50 == 0:
                temp_preds = torch.cat(all_preds[-50:])
                temp_targets = torch.cat(all_targets[-50:])
                temp_mse, temp_mae, temp_r2 = compute_metrics(temp_preds, temp_targets)
                pbar.set_postfix({'mse': temp_mse, 'mae': temp_mae, 'r2': temp_r2})

    pbar.close()

    # Final metrics
    all_preds = torch.cat(all_preds)
    all_targets = torch.cat(all_targets)
    epoch_mse, epoch_mae, epoch_r2 = compute_metrics(all_preds, all_targets)

    return {
        'loss': total_loss / len(loader.dataset),
        'mse': epoch_mse,
        'mae': epoch_mae,
        'r2': epoch_r2
    }


if __name__ == '__main__':
    # split
    idx = np.random.permutation(len(df))
    n80, n99 = int(.8 * len(df)), int(.99 * len(df))
    df_tr, df_va, df_te = df.iloc[idx[:n80]], df.iloc[idx[n80:n99]], df.iloc[idx[n99:]]

    tr_ds = MoleculeDataset(df_tr, conformers_list, atom_fs, bond_fs, tokenizer, y_mean, y_std)
    va_ds = MoleculeDataset(df_va, conformers_list, atom_fs, bond_fs, tokenizer, y_mean, y_std)
    te_ds = MoleculeDataset(df_te, conformers_list, atom_fs, bond_fs, tokenizer, y_mean, y_std)

    tr_ld = DataLoader(tr_ds, batch_size=32, shuffle=True,
                       collate_fn=collate_fn, pin_memory=True, num_workers=0)
    va_ld = DataLoader(va_ds, batch_size=32, shuffle=False,
                       collate_fn=collate_fn, pin_memory=True, num_workers=0)
    te_ld = DataLoader(te_ds, batch_size=32, shuffle=False,
                       collate_fn=collate_fn, pin_memory=True, num_workers=0)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = MultiModalRegressor(
        atom_dim=atom_fs.dim + 1,
        bond_dim=bond_fs.dim,
        vocab_size=tokenizer.vocab_size
    ).to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=10
    )
    swa_model = AveragedModel(model)
    scaler = torch.cuda.amp.GradScaler()

    best_val_loss = float('inf')
    patience_counter = 0

    # Store all metrics
    metrics_history = {
        'train': {'loss': [], 'mse': [], 'mae': [], 'r2': []},
        'val': {'loss': [], 'mse': [], 'mae': [], 'r2': []}
    }

    for epoch in range(1, 101):
        tr_metrics = train_epoch(model, tr_ld, optimizer, scaler, accum=2, dev=device, debug=(epoch == 1))
        va_metrics = eval_epoch(model, va_ld, device)

        scheduler.step(va_metrics['mse'])

        # Store metrics
        for split, metrics in [('train', tr_metrics), ('val', va_metrics)]:
            for metric_name, value in metrics.items():
                metrics_history[split][metric_name].append(value)

        # Print comprehensive metrics
        print(f'Epoch {epoch:03d} | '
              f'Train: Loss={tr_metrics["loss"]:.4f}, MSE={tr_metrics["mse"]:.4f}, '
              f'MAE={tr_metrics["mae"]:.4f}, R²={tr_metrics["r2"]:.4f} | '
              f'Val: Loss={va_metrics["loss"]:.4f}, MSE={va_metrics["mse"]:.4f}, '
              f'MAE={va_metrics["mae"]:.4f}, R²={va_metrics["r2"]:.4f}')

        if np.isnan(tr_metrics['loss']) or np.isnan(va_metrics['loss']):
            print("NaN detected in training! Stopping...")
            break

        model.save('last.pt', optimizer, scheduler)
        if va_metrics['loss'] < best_val_loss:
            best_val_loss = va_metrics['loss']
            patience_counter = 0
            model.save('best.pt', optimizer, scheduler)
        else:
            patience_counter += 1
            if patience_counter > 20:
                print('Early stopping.')
                break

        if epoch > 50:
            swa_model.update_parameters(model)

    update_bn(tr_ld, swa_model)
    torch.save(swa_model.state_dict(), 'swa.pt')

    # Save all metrics
    np.savez('metrics_history.npz', **{
        f'{split}_{metric}': np.array(values)
        for split, split_metrics in metrics_history.items()
        for metric, values in split_metrics.items()
    })

    # Final test evaluation
    best_model, _, _ = MultiModalRegressor.load(
        'best.pt', device,
        atom_dim=atom_fs.dim + 1, bond_dim=bond_fs.dim,
        vocab_size=tokenizer.vocab_size
    )
    test_metrics = eval_epoch(best_model, te_ld, device)

    # Denormalize for interpretation
    test_mse_denorm = test_metrics['mse'] * (y_std ** 2)
    test_mae_denorm = test_metrics['mae'] * y_std

    print(f'\n=== FINAL TEST RESULTS ===')
    print(f'Normalized - MSE: {test_metrics["mse"]:.4f}, MAE: {test_metrics["mae"]:.4f}, R²: {test_metrics["r2"]:.4f}')
    print(
        f'Original Scale - MSE: {test_mse_denorm:.4f}, MAE: {test_mae_denorm:.4f}, RMSE: {np.sqrt(test_mse_denorm):.4f}')
